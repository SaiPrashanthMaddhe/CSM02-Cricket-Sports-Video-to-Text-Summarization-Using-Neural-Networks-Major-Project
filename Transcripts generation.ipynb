{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "VGG16 Model"
      ],
      "metadata": {
        "id": "l6RXQFN1Bcbe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yowtCHN1BQCF"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from pickle import dump\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model\n",
        "\n",
        "# extract features from each photo in the directory\n",
        "def extract_features(directory):\n",
        "    model = VGG16()       # load the model\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)    # re-structure the model\n",
        "    print(model.summary())     # summarize\n",
        "    features = dict()        # extract features from each photo\n",
        "    for name in listdir(directory):\n",
        "        filename = directory + '/' + name      # load an image from file\n",
        "        image = load_img(filename, target_size=(224, 224))\n",
        "        image = img_to_array(image)            # convert the image pixels to a numpy array\n",
        "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))       # reshape data for the model\n",
        "        image = preprocess_input(image)         # prepare the image for the VGG model\n",
        "        feature = model.predict(image, verbose=0)             # get features\n",
        "        image_id = name.split('.')[0]            # get image id\n",
        "        features[image_id] = feature           # store feature\n",
        "        print('>%s' % name)\n",
        "    return features\n",
        "\n",
        "# extract features from all images\n",
        "#Enter the directory path containing images\n",
        "directory = './Images Dataset/Images'\n",
        "features = extract_features(directory)\n",
        "print('Extracted Features: %d' % len(features))\n",
        "# save to file\n",
        "dump(features, open('features1.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transcriptions Processing"
      ],
      "metadata": {
        "id": "YKKWq-eXBxc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\tfile = open(filename, 'r')     # open the file as read only\n",
        "\ttext = file.read()             # read all text\n",
        "\tfile.close()                   # close the file\n",
        "\treturn text\n",
        "\n",
        "# extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "\tmapping = dict()\n",
        "\tfor line in doc.split('\\n'):        # process lines\n",
        "\t\ttokens = line.split()             # split line by white space\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]        # take the first token as the image id, the rest as the description\n",
        "\t\timage_id = image_id.split('.')[0]                   # remove filename from image id\n",
        "\t\timage_desc = ' '.join(image_desc)                   # convert description tokens back to string\n",
        "\t\tif image_id not in mapping:                         # create the list if needed\n",
        "\t\t\tmapping[image_id] = list()\n",
        "\t\tmapping[image_id].append(image_desc)                # store description\n",
        "\treturn mapping\n",
        "\n",
        "def clean_descriptions(descriptions):\n",
        "\ttable = str.maketrans('', '', string.punctuation)     # prepare translation table for removing punctuation\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor i in range(len(desc_list)):\n",
        "\t\t\tdesc = desc_list[i]\n",
        "\t\t\tdesc = desc.split()                               # tokenize\n",
        "\t\t\tdesc = [word.lower() for word in desc]            # convert to lower case\n",
        "\t\t\tdesc = [w.translate(table) for w in desc]         # remove punctuation from each token\n",
        "\t\t\tdesc = [word for word in desc if len(word)>1]     # remove hanging 's' and 'a'\n",
        "\t\t\tdesc = [word for word in desc if word.isalpha()]  # remove tokens with numbers in them\n",
        "\t\t\tdesc_list[i] =  ' '.join(desc)                    # store as string\n",
        "\n",
        "# convert the loaded descriptions into a vocabulary of words\n",
        "def to_vocabulary(descriptions):\n",
        "\tall_desc = set()                              # build a list of all description strings\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        "\n",
        "# save descriptions to file, one per line\n",
        "def save_descriptions(descriptions, filename):\n",
        "\tlines = list()\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\tlines.append(key + ' ' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "\n",
        "#Enter the the commentary text file with path\n",
        "filename = './n_token.txt'\n",
        "doc = load_doc(filename)                          # load descriptions\n",
        "descriptions = load_descriptions(doc)             # parse descriptions\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "clean_descriptions(descriptions)                  # clean descriptions\n",
        "vocabulary = to_vocabulary(descriptions)          # summarize vocabulary\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\n",
        "save_descriptions(descriptions, 'descriptions1.txt')    # save to file"
      ],
      "metadata": {
        "id": "E5Vw7lupBrIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapping Frames to features - training"
      ],
      "metadata": {
        "id": "fE1Q5u2FB6Nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\tfile = open(filename, 'r')     # open the file as read only\n",
        "\ttext = file.read()             # read all text\n",
        "\tfile.close()                   # close the file\n",
        "\treturn text\n",
        "\n",
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\tfor line in doc.split('\\n'):         # process line by line\n",
        "\t\tif len(line) < 1:                  # skip empty lines\n",
        "\t\t\tcontinue\n",
        "\t\tidentifier = line.split('.')[0]    # get the image identifier\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)\n",
        "\n",
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\tdoc = load_doc(filename)                       \t# load document\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\ttokens = line.split()                         # split line by white space\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]  # split id from description\n",
        "\t\tif image_id in dataset:                       # skip images not in the set\n",
        "\t\t\tif image_id not in descriptions:            # create list\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'   # wrap description in tokens\n",
        "\t\t\tdescriptions[image_id].append(desc)                     # store\n",
        "\treturn descriptions\n",
        "\n",
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "\tall_features = load(open(filename, 'rb'))             # load all features\n",
        "\tfeatures = {k: all_features[k] for k in dataset}      # filter features\n",
        "\treturn features\n",
        "\n",
        "# load training dataset\n",
        "filename = './n_token.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "train_descriptions = load_clean_descriptions('descriptions1.txt', train)     # descriptions\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "train_features = load_photo_features('features1.pkl', train)                  # photo features\n",
        "print('Photos: train=%d' % len(train_features))"
      ],
      "metadata": {
        "id": "w6ktftUDB2AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Captioning Model"
      ],
      "metadata": {
        "id": "Os03ftHqCIta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "#from keras.layers.merge import add\n",
        "from keras.layers import Add\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "dVLocGFMCB5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\tfile = open(filename, 'r')      # open the file as read only\n",
        "\ttext = file.read()              # read all text\n",
        "\tfile.close()                    # close the file\n",
        "\treturn text"
      ],
      "metadata": {
        "id": "WBhOrPhSCLVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\tfor line in doc.split('\\n'):        # process line by line\n",
        "\t\tif len(line) < 1:                 # skip empty lines\n",
        "\t\t\tcontinue\n",
        "\t\tidentifier = line.split('.')[0]   # get the image identifier\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)"
      ],
      "metadata": {
        "id": "Z5ilt41QCOTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_clean_descriptions(filename, dataset):\n",
        "\tdoc = load_doc(filename)                                      # load document\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\ttokens = line.split()                                       # split line by white space\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]                # split id from description\n",
        "\t\tif image_id in dataset:                                     # skip images not in the set\n",
        "\t\t\tif image_id not in descriptions:                          # create list\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'     # wrap description in tokens\n",
        "\t\t\tdescriptions[image_id].append(desc)                       \t# store\n",
        "\treturn descriptions"
      ],
      "metadata": {
        "id": "t-NLEmdfCR2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "\tall_features = load(open(filename, 'rb'))            # load all features\n",
        "\tfeatures = {k: all_features[k] for k in dataset}     # filter features\n",
        "\treturn features"
      ],
      "metadata": {
        "id": "nhcBK34ZCSzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc"
      ],
      "metadata": {
        "id": "ypNOZRJICVQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# calculate the length of the description with the most words\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)\n",
        "\n",
        "# create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\tfor key, desc_list in descriptions.items():            # walk through each image identifier\n",
        "\t\tfor desc in desc_list:                               # walk through each description for the image\n",
        "\t\t\tseq = tokenizer.texts_to_sequences([desc])[0]      # encode the sequence\n",
        "\t\t\tfor i in range(1, len(seq)):                       # split one sequence into multiple X,y pairs\n",
        "\t\t\t\tin_seq, out_seq = seq[:i], seq[i]                # split into input and output pair\n",
        "\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]             # pad input sequence\n",
        "\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]     # encode output sequence\n",
        "\t\t\t\tX1.append(photos[key][0])                                       \t\t# store\n",
        "\t\t\t\tX2.append(in_seq)\n",
        "\t\t\t\ty.append(out_seq)\n",
        "\treturn array(X1), array(X2), array(y)\n",
        "\n",
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "\tinputs1 = Input(shape=(4096,))                            # feature extractor model\n",
        "\tfe1 = Dropout(0.5)(inputs1)\n",
        "\tfe2 = Dense(256, activation='relu')(fe1)\n",
        "\tinputs2 = Input(shape=(max_length,))                      # sequence model\n",
        "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "\tse2 = Dropout(0.5)(se1)\n",
        "\tse3 = LSTM(256)(se2)\n",
        "\tdecoder1 = Add()([fe2, se3])                                 # decoder model\n",
        "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
        "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)          # tie it together [image, seq] [word]\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\tprint(model.summary())                                             # summarize model\n",
        "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
        "\treturn model\n",
        "\n",
        "# train dataset\n",
        "\n",
        "# load training dataset\n",
        "filename = './Ntrain.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "train_descriptions = load_clean_descriptions('descriptions1.txt', train)       # descriptions\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "train_features = load_photo_features('features1.pkl', train)                   # photo features\n",
        "print('Photos: train=%d' % len(train_features))\n",
        "tokenizer = create_tokenizer(train_descriptions)                                                                                 # prepare tokenizer\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "max_length = max_length(train_descriptions)                                                                                      # determine the maximum sequence length\n",
        "print('Description Length: %d' % max_length)\n",
        "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)               # prepare sequences\n",
        "\n",
        "# dev dataset\n",
        "\n",
        "# load test set\n",
        "filename ='./Ntest.txt'\n",
        "test = load_set(filename)\n",
        "print('Dataset: %d' % len(test))\n",
        "test_descriptions = load_clean_descriptions('descriptions1.txt', test)           # descriptions\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "test_features = load_photo_features('features1.pkl', test)                       # photo features\n",
        "print('Photos: test=%d' % len(test_features))\n",
        "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features, vocab_size)                      # prepare sequences\n",
        "\n",
        "# fit model\n",
        "\n",
        "# define the model\n",
        "model = define_model(vocab_size, max_length)\n",
        "# fil//epath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'           # define checkpoint callback\n",
        "# checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, validation_data=([X1test, X2test], ytest))        # fit model\n"
      ],
      "metadata": {
        "id": "6tlfgHW4CatB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"Final_model.h5\")"
      ],
      "metadata": {
        "id": "MEkwhIA1CbuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "\t# seed the generation process\n",
        "\tin_text = 'startseq'\n",
        "\t# iterate over the whole length of the sequence\n",
        "\tfor i in range(max_length):\n",
        "\t\t# integer encode input sequence\n",
        "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# pad input\n",
        "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n",
        "\t\t# predict next word\n",
        "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
        "\t\t# convert probability to integer\n",
        "\t\tyhat = argmax(yhat)\n",
        "\t\t# map integer to word\n",
        "\t\tword = word_for_id(yhat, tokenizer)\n",
        "\t\t# stop if we cannot map the word\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\t# append as input for generating the next word\n",
        "\t\tin_text += ' ' + word\n",
        "\t\t# stop if we predict the end of the sequence\n",
        "\t\tif word == 'endseq':\n",
        "\t\t\tbreak\n",
        "\treturn in_text"
      ],
      "metadata": {
        "id": "Y0gcU-F5Cd-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "\tactual, predicted = list(), list()\n",
        "\t# step over the whole set\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# generate description\n",
        "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "\t\t# store actual and predicted\n",
        "\t\treferences = [d.split() for d in desc_list]\n",
        "\t\tactual.append(references)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "metadata": {
        "id": "8F5Ww-ndCi3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from numpy import argmax\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "# calculate the length of the description with the most words\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)\n",
        "\n",
        "# prepare tokenizer on train set\n",
        "\n",
        "# load training dataset (6K)\n",
        "filename = './Ntrain.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions('descriptions1.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "max_length = max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)\n",
        "\n",
        "# prepare test set\n",
        "\n",
        "# load test set\n",
        "filename = './Ntest.txt'\n",
        "test = load_set(filename)\n",
        "print('Dataset: %d' % len(test))\n",
        "# descriptions\n",
        "test_descriptions = load_clean_descriptions('descriptions1.txt', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "# photo features\n",
        "test_features = load_photo_features('features1.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))\n",
        "\n",
        "# load the model\n",
        "#Enter the path of saved model with lowest loss value\n",
        "filename = 'C:\\\\Users\\\\SAI PRASHANTH\\\\OneDrive\\\\Desktop\\\\MP Git\\\\Automated-Cricket-Commentary-\\\\model-ep011-loss0.377-val_loss0.484.h5'\n",
        "model = load_model(filename)\n",
        "# evaluate model\n",
        "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
      ],
      "metadata": {
        "id": "Y3EscGWDComQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from pickle import dump\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\t# process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# get the image identifier\n",
        "\t\tidentifier = line.split('.')[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)\n",
        "\n",
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\t# load document\n",
        "\tdoc = load_doc(filename)\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# split id from description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# skip images not in the set\n",
        "\t\tif image_id in dataset:\n",
        "\t\t\t# create list\n",
        "\t\t\tif image_id not in descriptions:\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\t# wrap description in tokens\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "\t\t\t# store\n",
        "\t\t\tdescriptions[image_id].append(desc)\n",
        "\treturn descriptions\n",
        "\n",
        "# covert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        "\n",
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# load training dataset (6K)\n",
        "filename = #'Ntrain.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions('descriptions1.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('tokenizer1.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "LkAVYmT8Cqu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z1_Ed_O1CyOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vlOMLvT7C1im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " For Frame Transcript Generation\n"
      ],
      "metadata": {
        "id": "ANmS3VvdC2Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# extract features from each photo in the directory\n",
        "def extract_features(filename):\n",
        "\t# load the model\n",
        "\tmodel = VGG16()\n",
        "\t# re-structure the model\n",
        "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "\t# load the photo\n",
        "\timage = load_img(filename, target_size=(224, 224))\n",
        "\t# convert the image pixels to a numpy array\n",
        "\timage = img_to_array(image)\n",
        "\t# reshape data for the model\n",
        "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t# prepare the image for the VGG model\n",
        "\timage = preprocess_input(image)\n",
        "\t# get features\n",
        "\tfeature = model.predict(image, verbose=0)\n",
        "\treturn feature\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "\t# seed the generation process\n",
        "\tin_text = 'startseq'\n",
        "\t# iterate over the whole length of the sequence\n",
        "\tfor i in range(max_length):\n",
        "\t\t# integer encode input sequence\n",
        "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# pad input\n",
        "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n",
        "\t\t# predict next word\n",
        "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
        "\t\t# convert probability to integer\n",
        "\t\tyhat = argmax(yhat)\n",
        "\t\t# map integer to word\n",
        "\t\tword = word_for_id(yhat, tokenizer)\n",
        "\t\t# stop if we cannot map the word\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\t# append as input for generating the next word\n",
        "\t\tin_text += ' ' + word\n",
        "\t\t# stop if we predict the end of the sequence\n",
        "\t\tif word == 'endseq':\n",
        "\t\t\tbreak\n",
        "\treturn in_text\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer1.pkl', 'rb'))\n",
        "# pre-define the max sequence length (from training)\n",
        "max_length = 25\n",
        "# load the model\n",
        "model = load_model('Final_model.h5')\n",
        "# load and prepare the photograph\n",
        "photo = extract_features('/content/1 - Copy.png')\n",
        "img = plt.imread(r'/content/1 - Copy.png')\n",
        "plt.imshow(img)\n",
        "# generate description\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(description)"
      ],
      "metadata": {
        "id": "gcTqe5ImCs5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For Continuous Frames Transcript generation"
      ],
      "metadata": {
        "id": "aFZhYs-XDLtw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8xyIgBelDNnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "#To test\n",
        "def test(directory):\n",
        "  tokenizer = load(open('tokenizer1.pkl', 'rb'))\n",
        "  max_length = 25\n",
        "  model = load_model('Final_model.h5')\n",
        "  lis = []\n",
        "  list1 = os.listdir(directory)\n",
        "  for i in list1:\n",
        "    name = directory + \"/\" + i\n",
        "    photo = extract_features(name)\n",
        "    img = plt.imread(name)\n",
        "    plt.imshow(img)\n",
        "    # generate description\n",
        "    description = generate_desc(model, tokenizer, photo, max_length)\n",
        "    print(description)\n",
        "    lis.append([i,description])\n",
        "  return lis\n",
        "\n",
        "#Enter the directory containg images to get captioning\n",
        "directory = \"\"\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JQjid-WWDK4V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}